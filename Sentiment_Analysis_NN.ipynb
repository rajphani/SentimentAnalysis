{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis NN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "w5v7NjwBbH1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "hm_lines = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y05HR9JQbH1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_lexicon(pos,neg):\n",
        "\n",
        "    lexicon = []\n",
        "    with open(pos,'r') as f:\n",
        "        contents = f.readlines()\n",
        "        for l in contents[:hm_lines]:\n",
        "            all_words = word_tokenize(l)\n",
        "            lexicon += list(all_words)\n",
        "\n",
        "    with open(neg,'r') as f:\n",
        "        contents = f.readlines()\n",
        "        for l in contents[:hm_lines]:\n",
        "            all_words = word_tokenize(l)\n",
        "            lexicon += list(all_words)\n",
        "\n",
        "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
        "    w_counts = Counter(lexicon)\n",
        "    l2 = []\n",
        "    for w in w_counts:\n",
        "        #print(w_counts[w])\n",
        "        if 1000 > w_counts[w] > 50:\n",
        "            l2.append(w)\n",
        "            print(len(l2))\n",
        "        return l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wuiOsConbH1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_handling(sample,lexicon,classification):\n",
        "\n",
        "    featureset = []\n",
        "\n",
        "    with open(sample,'r') as f:\n",
        "        contents = f.readlines()\n",
        "        for l in contents[:hm_lines]:\n",
        "            current_words = word_tokenize(l.lower())\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            features = np.zeros(len(lexicon))\n",
        "            for word in current_words:\n",
        "                if word.lower() in lexicon:\n",
        "                    index_value = lexicon.index(word.lower())\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            featureset.append([features,classification])\n",
        "\n",
        "    return featureset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FBNPmSHIbH1r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n",
        "    lexicon = create_lexicon(pos,neg)\n",
        "    features = []\n",
        "    features += sample_handling('pos.txt',lexicon,[1,0])\n",
        "    features += sample_handling('neg.txt',lexicon,[0,1])\n",
        "    random.shuffle(features)\n",
        "    features = np.array(features)\n",
        "\n",
        "    testing_size = int(test_size*len(features))\n",
        "\n",
        "    train_x = list(features[:,0][:-testing_size])\n",
        "    train_y = list(features[:,1][:-testing_size])\n",
        "    test_x = list(features[:,0][-testing_size:])\n",
        "    test_y = list(features[:,1][-testing_size:])\n",
        "\n",
        "    return train_x,train_y,test_x,test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1oqweyN_bH1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from create_sentiment_featuresets import create_feature_sets_and_labels\n",
        "import tensorflow as tf\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
        "\n",
        "n_nodes_hl1 = 1500\n",
        "n_nodes_hl2 = 1500\n",
        "n_nodes_hl3 = 1500\n",
        "\n",
        "n_classes = 2\n",
        "batch_size = 100\n",
        "hm_epochs = 10\n",
        "\n",
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
        "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "\n",
        "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
        "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "\n",
        "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
        "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
        "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
        "\n",
        "output_layer = {'f_fum':None,\n",
        "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
        "                'bias':tf.Variable(tf.random_normal([n_classes])),}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XMiO00ocbH1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_network_model(data):\n",
        "\n",
        "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "\n",
        "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "\n",
        "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "\n",
        "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEKZtNSbbH1y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(x):\n",
        "    prediction = neural_network_model(x)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        \n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            i=0\n",
        "            while i < len(train_x):\n",
        "                start = i\n",
        "                end = i+batch_size\n",
        "                batch_x = np.array(train_x[start:end])\n",
        "                batch_y = np.array(train_y[start:end])\n",
        "\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
        "                                                              y: batch_y})\n",
        "                epoch_loss += c\n",
        "                i+=batch_size\n",
        "        \n",
        "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "\n",
        "        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BGGkl_Z1bH10",
        "colab_type": "code",
        "colab": {},
        "outputId": "577407d6-62c6-4c99-f10d-20d7773b12df"
      },
      "cell_type": "code",
      "source": [
        "train_neural_network(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 10 loss: 238992.35917282104\n",
            "Epoch 2 completed out of 10 loss: 208219.97452545166\n",
            "Epoch 3 completed out of 10 loss: 137007.53738427162\n",
            "Epoch 4 completed out of 10 loss: 195916.18661499023\n",
            "Epoch 5 completed out of 10 loss: 84083.68679046631\n",
            "Epoch 6 completed out of 10 loss: 140742.9299545288\n",
            "Epoch 7 completed out of 10 loss: 140000.22305434942\n",
            "Epoch 8 completed out of 10 loss: 148997.50070095062\n",
            "Epoch 9 completed out of 10 loss: 109000.12267684937\n",
            "Epoch 10 completed out of 10 loss: 179201.36331367493\n",
            "Accuracy: 0.50093806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5ALfC94bH15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}